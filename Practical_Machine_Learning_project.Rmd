---
output: 
  html_document: 
    fig_caption: yes
    fig_height: 5.5
    fig_width: 9
    keep_md: yes
    sansfont: Calibri Light
---
#**Predictive Machine Learning Project **    
### *Predictive Analysis of the Quality of Weight Lifting Exercises*  
<p></br></p>

####**Tihana Mirkovic**
###### September 28, 2016  

<p></br></p>


##**1 Introduction (Synopsis)**

Owing to the advancements in wearable technology for the tracking of personal activity, such as Jawbone Up, Nike FuelBand, and Fitbit, a large amount of data on physical performance is easily collected. The primary goal of these devices is to allow customers to quantify self movement, but often, the quality of the exercise performance gets neglected. In this project, data collected from 6 individuals while performing barbell lifts from accelerometers on the belt, forearm, arm, as well as the dumbbell itself, is used in order to build a predictive model for the classification of how well the weightlifting exercise was performed. 

The quality of the execution of the weightlifting exercise was defined in five different categories:  

*  **CLASS A**: exactly according to the specification  
*  **CLASS B**: throwing the elbows to the front  
*  **CLASS C**: lifting the dumbbell only halfway
*  **CLASS D**: lowering the dumbbell only halfway
*  **CLASS E**: throwing the hips to the front

A [training data set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and a [test data set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) have been obtained from the [course project website](https://www.coursera.org/learn/practical-machine-learning/peer/R43St/prediction-assignment-writeup) with further information provided on the original [source website](http://groupware.les.inf.puc-rio.br/har).

The machine learning algorithm developed in this project is outlined below. The classification tree approach, the generalized boosted model, the random forest classification were compared and assessed for accuracy. Consequently the most accurate model, in this case the random forest classification, was applied to the test data set in order to obtain predictions in which manner the exercises were performed in (class A to E).

<p></br></p>



##**2 Data Processing**


####**2.1 R Document Set Up**

- The necessary libraries for the analysis and graphing were loaded.
- Also, `echo = TRUE` has been selected for all the code chunks, so that they are visible to reviewers.

```{r results='hide', message=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev="png", dpi=300)
library(caret)
library(randomForest)
library(lattice)
library(ggplot2)
library(gbm)
library(rpart)
library(MASS)
library(plyr)
library(corrplot)
library(rattle)
library(rpart.plot)
library(htmlTable)
library(gridExtra)
library(cowplot)
set.seed(1234)
```
<p></br></p>

####**2.2 Downloading the Data**  

First, the two data sets, the training and the test data set, were downloaded from the course website, and subsequently `read.csv` was used to read them in. 
```{r}
##downloading training data set
if(!file.exists("./pml_data_training.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile="./pml_data_training.csv")
}
training_all<-read.csv("pml_data_training.csv", na.strings=c('#DIV/0', '', 'NA') ,stringsAsFactors = F, header = TRUE)


##downloading testing data set
if(!file.exists("./pml_data_testing.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile="./pml_data_testing.csv")
}
testing_all<-read.csv("pml_data_testing.csv",na.strings=c('#DIV/0', '', 'NA') ,stringsAsFactors = F, header = TRUE)
```
<p></br></p>

####**2.2 Initial Data Check**  

The dimensions of the two sets are checked, and the names of the columns compared. Only the last column, `classe` in the training set, and `problem_id` in the test set, have different names.
```{r}
##checking dimensions
dim(training_all)
dim(testing_all)

##checking if all the columns are named the same
names(training_all) %in% names(testing_all)

##the names of the two columns that differ in the training/test sets
names(training_all[160])
names(testing_all[160])
```

<p></br></p>

####**2.3 Data Cleaning**  

A number of columns contain a large number missing values. The number of NA occurrences for each column is determined, and the names of those with above 19000 NA values have been selected to be removed from the data sets. 
```{r}
##counting NAs in each column and finding the names of those columns
na_count <-sapply(training_all, function(y) sum(length(which(is.na(y)))))
na_columns<-subset(na_count, na_count>19000)
names_na_columns<-names(na_columns)

```
<p></br></p>


Then, the NA columns, as well as the first seven columns, which don't serve as predictor variables are removed from the data set, for both, the `testing_all` and the `training_all` data sets, creating the `training_1` and the `testing_1` data sets . In the case of the training data set, the `classe` variable is reset to `factor` using the `as.factor` function. The `dim` function confirms that the new data sets contain 53 columns (52 predictors and `classe`/`problem_id`) and 19622 rows for the training and 20 rows for the testing data set. 
```{r}

training_2 <- training_all[, -c(1:7)]
training_1<- training_2[, -which(names(training_2)%in%names_na_columns)]
training_1$classe<-as.factor(training_1$classe)


testing_2 <- testing_all[, -c(1:7)]
testing_1<-testing_2[, -which(names(training_2)%in%names_na_columns)]

dim(training_1)
dim(testing_1)

```
<p></br></p>


####**2.4 Data Splitting** 

The testing data set will be used for the final run, so we need to split the cleaned `training_1` data set into a new `training` (70%) and `validation` (30%) data set. The `validation` data set will be used to test the out-of-sample error.
```{r}
set.seed(1234)
inTrain<-createDataPartition(training_1$classe, p=0.7, list=FALSE)
training<-training_1[inTrain,]
validation<-training_1[-inTrain,]

dim(training)
dim(validation)
```
<p></br></p>

##**3 Model Creation**


####**3.1 Variable Selection - Correlation Analysis**
There are 52 predictor variables, and the correlation was checked to see if there are any that are highly correlated variables (with a cut off set at 0.9) which would allow us to reduce their number in the predictive model. A smaller training set was then produced, taking out the highly correlated variables and creating `training_small`, with a total of 46 columns. Similar reduction in data sets was then also performed for the `validation` and `test` data sets. 
```{r}

correlationMatrix<-cor(training[,1:52])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.90)

training_small<-training[,-highlyCorrelated]
validation_small<-validation[,-highlyCorrelated]
test_small<-testing_1[,-highlyCorrelated]

dim(training_small)
```

A correlation matrix is plotted to better visualize the correlated variables, as well as the changes upon removing some of them to create the `training_small` data set. 
```{r}

correlationMatrix_small<-cor(training_small[,-46])

par(mfrow = c(1, 2))

##correlation Matrix for the original clean training set
corrplot(correlationMatrix, type="lower", order="FPC", method="color", tl.cex = 0.4, tl.col = "black")

##correlation Matrix for the smaller training set with some of the highly correlated variables removed       
corrplot(correlationMatrix_small, type="lower", order="FPC", method="color", tl.cex = 0.4, tl.col = "black")

```

**Figure 1**   The correlation matrix of the  clean training set, `training` (left), and the correlation matrix `training_small` produced after the removal of the seven variables that were found to have a high correlation (right). The variables that are highly correlated are shown in the dark hues of blue and red. 

<p></br></p>

####**3.2 Variable Selection - Principle Component Analysis**
In the previous section we identified a few highly correlated variables, but an additional preprocessing step can be performed by doing the Principal Component Analysis (PCA), which could further decrease the number of components needed. 

```{r}
preProc_training<-preProcess(training[,-53], method="pca", thresh=0.95)
preProc_training
training_pca<-predict(preProc_training,training[,-53])
dim(training_pca)
```
Above, we can see that PCA has yielded a set of 25 components, substantially reducing the original number of predictor variables from 52, that are able to capture 95% of the variance of the data set. 
<p></br></p>

####**3.3 Data Set Summary and Selection**

Below, in **Table 1**, all the different data sets which have been created so far are summarized in order to help us determine the data sets we are going to use in our model development.
```{r}
##Creating a table with all the data set information
data_set<-c("training", "training_small", "training_pca", "validation", "validation_small", "testing_1")
dimensions<-c("13737 x 53", "13737 x 46","13737 x 25","5885 x 53","5885 x 46","20 x 53")
origin<-c("partitioned training set without NAs", "from `training` without highly correlated variables",
          "PCA performed on `training`","from partitioned training set without NAs",
          "from `validation` withouth highly correlated variables",
          "cleaned testing set matching `training`")
Tab_data_sets = as.table(cbind(data_set, dimensions, origin))

htmlTable(Tab_data_sets, align="l|l|l|", rnames=c("", "", "", "", "", ""), caption="**Table 1 ** Summary of the data sets and their respective dimensions")
```
  
<p></br></p>
The advantages of using the `training_pca` set would be that it is more compact and would allow for a faster analysis, but the disadvantage is that the predictor variables used in the training set derived through PCA are not easily interpretable.   

Here, as the number of variables in the `training` data set is still manageable. We will continue with that data set, in order to relate the results to the physical concepts of the experiment. If prediction and calculation time were the only factors to consider, then the reduced `training_pca` data set can be used. 

Performing the modelling with the `training_small` set was also done as a check (not shown here because of assignment length requirements), and no noticeable differences were observed compared to the `training` set.
<p></br></p>

####**3.4 Model Selection: Classification Tree**

A classification tree approach was used on the `training` data set and the developed model was subsequently applied on the `validation` set to check the model accuracy. 
```{r, cache=TRUE}
set.seed(1234)
modFit_tree<-train(classe~., method="rpart", data=training)
print(modFit_tree$finalModel)
modFit_tree_2 <- rpart(classe~., data=training, method="class")
```

```{r}
fancyRpartPlot(modFit_tree$finalModel)
```
**Figure 2**   A Classification Tree dendrogram.
<p></br></p>
<p></br></p>
Below, the accuracy of the classification tree approach is calculated on the predictions of the `validation` data set, and with 0.6879 accuracy, it is proven not to be good enough model for our prediction analysis. 
```{r, cache=TRUE}
##testing the Classification tree model
predictions_tree<-predict(modFit_tree_2, validation, type="class")
C_Matrix_tree<-confusionMatrix(predictions_tree, validation$classe)
C_Matrix_tree
```

<p></br></p>


####**3.5 Model Selection: Generalized Boosted Model**

Then the Generalized Boosted Model was applied to the `training` and `validation` data sets. Here, we use a 3-fold cross validation due to the high computational time. 
```{r, cache=TRUE}
set.seed(1234)
control_gbm<-trainControl(method='cv', number=3) #k-fold Cross validation
modFit_gbm<-train(classe~., method="gbm", data=training, trControl=control_gbm, verbose=FALSE)
modFit_gbm$finalModel
```

```{r, cache=TRUE}
##testing the gbm model
predictions_gbm<-predict(modFit_gbm, newdata=validation)
C_Matrix_gbm<-confusionMatrix(validation$classe,predict(modFit_gbm, validation))
C_Matrix_gbm
```
<p></br></p>
The GBM model has achieved a 96.33% accuracy rate on the prediction for the `validation` set. 

<p></br></p>
####**3.6 Model Selection: Random Forest**

Then, the Random forest approach was applied to the `training` and `validation` data sets. Here, we use a 5-fold cross validation due to the high computational time. 
```{r, cache=TRUE}
set.seed(1234)
control_rf<-trainControl(method="cv", 5) #k-fold Cross validation
modFit_rf<-train(classe~., method="rf", data=training, trControl=control_rf, ntree=200)
```

```{r, cache=TRUE}
##testing the random forest model
predictions_rf<-predict(modFit_rf, newdata=validation)
C_Matrix_rf<-confusionMatrix(validation$classe,predict(modFit_rf, validation))
C_Matrix_rf
```


<p></br></p>
####**3.7 Final Model Selection and Out-of-Sample Error**
Out of the three models that were investigated, the random forest model showed the highest accuracy, as depicted in **Figure 3**. 

```{r}

layout(t(1:3))
plot(C_Matrix_tree$table, col="grey", main =paste("Classification Tree Model \nAccuracy=", round(C_Matrix_tree$overall['Accuracy'],4),"\nOut of Sample Error=",1-round(C_Matrix_tree$overall['Accuracy'],4)))

plot(C_Matrix_gbm$table, col="grey", main =paste("Generalized Boosted Model \nAccuracy=", round(C_Matrix_gbm$overall['Accuracy'],4),"\nOut of Sample Error=",1-round(C_Matrix_gbm$overall['Accuracy'],4)))

plot(C_Matrix_rf$table, col="grey", main =paste("Random Forest Model \nAccuracy=", round(C_Matrix_rf$overall['Accuracy'],4),"\nOut of Sample Error=",1-round(C_Matrix_rf$overall['Accuracy'],5)))

```
**Figure 3**   The tables of the confusion matrices illustrate the accuracies of the three models that were investigated: **Classification Tree** (left), **Generalized Boosted Model** (centre), **Random Forest** (right). 
<p></br></p>

The random forest model will be the selected model with which the predictions on the test data set will be performed. **Figure 4** illustrates the factors with the highest importance in the random forest model. 
```{r}
v_imp<-varImp(modFit_rf)
plot(v_imp, top=20)
```
**Figure 4**   A depiction of the top 20 variables in the random forest model. 


<p></br></p>

##**4 Prediction on Test Data Set**
By applying the random forest model, the following predictions were made for the exercise categories of the test data set:

```{r, cache=TRUE}
predictions_rf_test<-predict(modFit_rf, newdata=testing_1)
predictions_rf_test
```
<p></br></p>



##**5 Conclusion**

An accurate prediction model was built for the classification of the quality of barbell lifts based on the variables collected from a number of accelerometers monitored during the exercise. In this study, the Classification Tree, the Generalized Boosted Model approach, and the Random Forest classification technique were explored, and it was found that the Random Forest Classification technique with a 5-fold cross validation, performed the best, with and an accuracy of 0.9944 and an out-of-sample error estimated at 0.0056. Consequently it was selected as the model that was used on the test data to predict the exercise classification. 

In the data cleaning and model building process, the training set size was explored. In the initial step the original data set was cleaned by removing columns with missing values. Also, alternative training sets were formed by either removing the top 7 highly correlated factors, or performing Principle Component Analysis, which reduced the required number of parameters even further. The smaller training data sets would be desirable if one considers calculation time to be an issue, as random forest requires a substantial amount of computing time. Here, the training set with 52 predictor variables was used, and variable importance evaluation in the different models has confirmed that `roll_belt`, and `pitch_forearm` tend to rank the highest.   


